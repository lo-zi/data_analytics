{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0f68ae5",
   "metadata": {},
   "source": [
    "# Natural Language Processing - Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2054b14d",
   "metadata": {},
   "source": [
    "## Libraries and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba4eb3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/vscode/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to /home/vscode/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to /home/vscode/nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to /home/vscode/nltk_data...\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/vscode/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /workspaces/data_analytics/Week_11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    }
   ],
   "source": [
    "# Libraries\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "import nltk\n",
    "\n",
    "# Import only once\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.chunk import tree2conlltags\n",
    "from nltk.chunk import conlltags2tree\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Current working directory\n",
    "print('Current working directory:', os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e252684a",
   "metadata": {},
   "source": [
    "## Defining documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e8057467",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The car is driven on the road. The truck is driven on the highway. The bicycle is driven on the bicycle path.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining documents (=sentenses)\n",
    "d1 = 'The car is driven on the road.'\n",
    "d2 = 'The truck is driven on the highway.'\n",
    "d3 = 'The bicycle is driven on the bicycle path.'\n",
    "\n",
    "corpus_01 = d1 + ' ' + d2 + ' ' + d3\n",
    "corpus_01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2884e42b",
   "metadata": {},
   "source": [
    "## Aufgabe b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3cad2b64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Das schöne, junge Schneewittchen wächst als Dienstmagd am Hof ihres Vaters und ihrer neidischen Stiefmutter auf.. Schneewittchen irrt voller Angst durch die Nacht und schläft schließlich ein.. Sie erwacht am nächsten Morgen im Kreise der Tiere des Waldes'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining documents (=sentenses)\n",
    "d1 = 'Das schöne, junge Schneewittchen wächst als Dienstmagd am Hof ihres Vaters und ihrer neidischen Stiefmutter auf..'\n",
    "d2 = 'Schneewittchen irrt voller Angst durch die Nacht und schläft schließlich ein..'\n",
    "d3 = 'Sie erwacht am nächsten Morgen im Kreise der Tiere des Waldes'\n",
    "\n",
    "corpus_01 = d1 + ' ' + d2 + ' ' + d3\n",
    "corpus_01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fadda5",
   "metadata": {},
   "source": [
    "## Text preprocessing\n",
    "#### Steps:\n",
    "- Text to lowercase\n",
    "- Removing punctuations\n",
    "- Tokenization\n",
    "- Removal of stop words\n",
    "- Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e649b8",
   "metadata": {},
   "source": [
    "### Text to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2666c8b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the car is driven on the road. the truck is driven on the highway. the bicycle is driven on the bicycle path.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text to lowercase function\n",
    "def text_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "# Text to lowercase\n",
    "corpus_02 = text_lowercase(corpus_01)\n",
    "corpus_02"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c837286f",
   "metadata": {},
   "source": [
    "### Removing punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90067406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the car is driven on the road the truck is driven on the highway the bicycle is driven on the bicycle path'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove punctuation function\n",
    "def remove_punctuation(text):\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "# Remove punctuation\n",
    "corpus_03 = remove_punctuation(corpus_02)\n",
    "corpus_03"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986153d2",
   "metadata": {},
   "source": [
    "### Tokenize text & removal of stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2e99fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of english stopwords:\n",
      "{\"hasn't\", 'we', \"haven't\", 'ain', 'both', 'it', 'be', 'that', 'being', 'for', \"weren't\", 'in', 'own', 'him', 'its', \"wasn't\", 'on', 'you', 'above', 'under', 'weren', 'by', 'when', 'very', 'shan', 've', 'only', 'a', 'any', 'my', 'itself', 'now', 'they', 're', 'once', 'or', 'why', \"mustn't\", 'their', 'no', 'too', 'themselves', \"she's\", 'off', 'same', \"you'll\", 't', 'if', 'd', 'needn', 'hasn', 'herself', 'then', 'have', 'how', 'what', 's', 'isn', \"needn't\", 'during', 'aren', \"it's\", 'her', 'not', 'up', 'further', 'does', \"shan't\", 'was', \"you'd\", 'theirs', \"shouldn't\", 'because', 'wouldn', 'o', 'over', 'i', 'yourselves', 'out', 'me', 'to', 'all', \"couldn't\", 'few', 'before', 'are', 'whom', 'of', 'mightn', 'do', 'as', \"didn't\", 'doesn', 'below', \"you're\", \"won't\", \"wouldn't\", 'shouldn', 'did', 'myself', 'can', \"don't\", \"that'll\", 'who', 'at', 'ma', \"doesn't\", 'y', 'each', 'just', 'won', 'yourself', 'from', 'than', 'your', \"mightn't\", 'with', 'himself', 'such', 'hadn', 'where', 'he', 'haven', 'were', 'his', 'didn', 'mustn', \"should've\", 'against', 'other', 'll', 'into', 'nor', 'doing', 'here', 'there', 'yours', \"you've\", \"aren't\", 'had', 'hers', 'but', 'after', 'between', 'is', 'while', 'again', 'having', \"hadn't\", 'this', 'which', 'she', 'ourselves', 'most', 'will', 'the', 'our', 'some', 'should', 'am', 'an', 'wasn', 'don', 'm', 'ours', 'has', 'until', 'these', 'and', \"isn't\", 'through', 'them', 'those', 'about', 'so', 'down', 'been', 'more', 'couldn'}\n"
     ]
    }
   ],
   "source": [
    "# Show english stopwords\n",
    "eng_stopwords = set(stopwords.words('english'))\n",
    "print(\"List of english stopwords:\")\n",
    "print(eng_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d83ab939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['car', 'driven', 'road', 'truck', 'driven', 'highway', 'bicycle', 'driven', 'bicycle', 'path']"
     ]
    }
   ],
   "source": [
    "# Function for tokenization and the removal of stopwords\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_text = [word for word in word_tokens if word not in stop_words]\n",
    "    return filtered_text\n",
    " \n",
    "# Remove stopwords\n",
    "corpus_04 = remove_stopwords(corpus_03)\n",
    "print(corpus_04, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad590183",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "410fed5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before lemmatization:\n",
      "['car', 'driven', 'road', 'truck', 'driven', 'highway', 'bicycle', 'driven', 'bicycle', 'path'] \n",
      "\n",
      "After lemmatization:\n",
      "['car', 'drive', 'road', 'truck', 'drive', 'highway', 'bicycle', 'drive', 'bicycle', 'path']"
     ]
    }
   ],
   "source": [
    "# Initialize Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize string function\n",
    "def lemmatize_word(text):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    lemmas = [lemmatizer.lemmatize(word, pos ='v') for word in word_tokens]\n",
    "    return lemmas\n",
    "\n",
    "# Lemmatize\n",
    "lem = []\n",
    "for i in corpus_04:\n",
    "    lem.append(lemmatize_word(i))\n",
    "\n",
    "# Nested list to list\n",
    "corpus_05 = [' '.join([str(x) for x in lst]) for lst in lem]\n",
    "\n",
    "print('Before lemmatization:')\n",
    "print(corpus_04, '\\n')\n",
    "\n",
    "print('After lemmatization:')\n",
    "print(corpus_05, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9ad6de",
   "metadata": {},
   "source": [
    "## Redefine the text corpus (pre-processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "08a3cb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the lemmatized words above to re-define our corpus \n",
    "corpus = ['das schöne junge schneewittchen wächst als dienstmagd am hof ihres vaters und ihrer neidischen stiefmutter auf',\n",
    "                  'schneewittchen irrt voller angst durch die nacht und schläft schließlich ein',\n",
    "                  'sie erwacht am nächsten morgen im kreise der tiere des waldes']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198cc6d0",
   "metadata": {},
   "source": [
    "## Document-term matrix with ngram_range=(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ead679d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document-term matrix\n",
      "   als  am  angst  auf  das  der  des  die  dienstmagd  durch  ...  \\\n",
      "0    1   1      0    1    1    0    0    0           1      0  ...   \n",
      "1    0   0      1    0    0    0    0    1           0      1  ...   \n",
      "2    0   1      0    0    0    1    1    0           0      0  ...   \n",
      "\n",
      "   schneewittchen  schöne  sie  stiefmutter  tiere  und  vaters  voller  \\\n",
      "0               1       1    0            1      0    1       1       0   \n",
      "1               1       0    0            0      0    1       0       1   \n",
      "2               0       0    1            0      1    0       0       0   \n",
      "\n",
      "   waldes  wächst  \n",
      "0       0       1  \n",
      "1       0       0  \n",
      "2       1       0  \n",
      "\n",
      "[3 rows x 35 columns]\n"
     ]
    }
   ],
   "source": [
    "# Vectorizer with ngram_range=(1,1)\n",
    "vectorizer = CountVectorizer(min_df=0.0, ngram_range=(1,1))\n",
    "\n",
    "# Transform \n",
    "count = vectorizer.fit_transform(corpus)\n",
    " \n",
    "# Create dataframe\n",
    "df_count = pd.DataFrame(count.toarray(),\n",
    "                        columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "print('Document-term matrix')\n",
    "print(df_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417feb3a",
   "metadata": {},
   "source": [
    "## Document-term matrix with ngram_range=(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4eb33ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document-term matrix\n",
      "   als dienstmagd  am hof  am nächsten  angst durch  das schöne  der tiere  \\\n",
      "0               1       1            0            0           1          0   \n",
      "1               0       0            0            1           0          0   \n",
      "2               0       0            1            0           0          1   \n",
      "\n",
      "   des waldes  die nacht  dienstmagd am  durch die  ...  \\\n",
      "0           0          0              1          0  ...   \n",
      "1           0          1              0          1  ...   \n",
      "2           1          0              0          0  ...   \n",
      "\n",
      "   schneewittchen wächst  schöne junge  sie erwacht  stiefmutter auf  \\\n",
      "0                      1             1            0                1   \n",
      "1                      0             0            0                0   \n",
      "2                      0             0            1                0   \n",
      "\n",
      "   tiere des  und ihrer  und schläft  vaters und  voller angst  wächst als  \n",
      "0          0          1            0           1             0           1  \n",
      "1          0          0            1           0             1           0  \n",
      "2          1          0            0           0             0           0  \n",
      "\n",
      "[3 rows x 35 columns]\n"
     ]
    }
   ],
   "source": [
    "# Vectorizer with with ngram_range=(2,2)\n",
    "vectorizer = CountVectorizer(min_df=0.0, ngram_range=(2,2))\n",
    "\n",
    "# Transform \n",
    "count = vectorizer.fit_transform(corpus)\n",
    " \n",
    "# Create dataframe\n",
    "df_count = pd.DataFrame(count.toarray(),\n",
    "                        columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "print('Document-term matrix')\n",
    "print(df_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c846a236",
   "metadata": {},
   "source": [
    "## Term frequency-inverse document frequency (TF-IDF)\n",
    "- For details see: https://www.learndatasci.com/glossary/tf-idf-term-frequency-inverse-document-frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5854fa81",
   "metadata": {},
   "source": [
    "### Term Frequency (TF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f9ff38f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in the corpus: 35 \n",
      "\n",
      "The words in the corpus: \n",
      " {'morgen', 'als', 'ihrer', 'ihres', 'schließlich', 'erwacht', 'kreise', 'und', 'stiefmutter', 'schneewittchen', 'am', 'schläft', 'im', 'tiere', 'neidischen', 'die', 'angst', 'junge', 'nacht', 'dienstmagd', 'ein', 'vaters', 'wächst', 'sie', 'schöne', 'der', 'irrt', 'das', 'auf', 'waldes', 'durch', 'des', 'hof', 'voller', 'nächsten'}\n",
      "\n",
      "Term Frequency (TF):\n",
      "   morgen     als   ihrer   ihres  schließlich  erwacht  kreise     und  \\\n",
      "0  0.0000  0.0625  0.0625  0.0625       0.0000   0.0000  0.0000  0.0625   \n",
      "1  0.0000  0.0000  0.0000  0.0000       0.0909   0.0000  0.0000  0.0909   \n",
      "2  0.0909  0.0000  0.0000  0.0000       0.0000   0.0909  0.0909  0.0000   \n",
      "\n",
      "   stiefmutter  schneewittchen  ...     der    irrt     das     auf  waldes  \\\n",
      "0       0.0625          0.0625  ...  0.0000  0.0000  0.0625  0.0625  0.0000   \n",
      "1       0.0000          0.0909  ...  0.0000  0.0909  0.0000  0.0000  0.0000   \n",
      "2       0.0000          0.0000  ...  0.0909  0.0000  0.0000  0.0000  0.0909   \n",
      "\n",
      "    durch     des     hof  voller  nächsten  \n",
      "0  0.0000  0.0000  0.0625  0.0000    0.0000  \n",
      "1  0.0909  0.0000  0.0000  0.0909    0.0000  \n",
      "2  0.0000  0.0909  0.0000  0.0000    0.0909  \n",
      "\n",
      "[3 rows x 35 columns]\n"
     ]
    }
   ],
   "source": [
    "# Compute Term Frequency (TF)\n",
    "words_set = set()\n",
    "for doc in corpus:\n",
    "    words = doc.split(' ')\n",
    "    words_set = words_set.union(set(words))\n",
    "    \n",
    "print('Number of words in the corpus:',len(words_set), '\\n')\n",
    "print('The words in the corpus: \\n', words_set)\n",
    "\n",
    "# Number of documents in the corpus\n",
    "n_docs = len(corpus)\n",
    "\n",
    "# Number of unique words in the corpus \n",
    "n_words_set = len(words_set)\n",
    "\n",
    "df_tf = pd.DataFrame(np.zeros((n_docs, n_words_set)), \n",
    "                     columns=list(words_set))\n",
    "\n",
    "print(\"\\nTerm Frequency (TF):\")\n",
    "for i in range(n_docs):\n",
    "    # Words in the document\n",
    "    words = corpus[i].split(' ')\n",
    "    for w in words:\n",
    "        df_tf[w][i] = df_tf[w][i] + (1 / len(words))\n",
    "        \n",
    "print(df_tf.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91dae3d",
   "metadata": {},
   "source": [
    "### Inverse Document Frequency (IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5fe31336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inverse Document Frequency (IDF):\n",
      "          truck:     0.4771\n",
      "           road:     0.4771\n",
      "        bicycle:     0.4771\n",
      "          drive:        0.0\n",
      "        highway:     0.4771\n",
      "            car:     0.4771\n",
      "           path:     0.4771\n"
     ]
    }
   ],
   "source": [
    "# Computing Inverse Document Frequency (IDF)\n",
    "print(\"\\nInverse Document Frequency (IDF):\")\n",
    "\n",
    "idf = {}\n",
    "\n",
    "for w in words_set:\n",
    "    \n",
    "    # k = number of documents that contain this word\n",
    "    k = 0\n",
    "    \n",
    "    for i in range(n_docs):\n",
    "        if w in corpus[i].split():\n",
    "            k += 1\n",
    "            \n",
    "    idf[w] =  np.log10(n_docs / k).round(4)\n",
    "    \n",
    "    print(f'{w:>15}: {idf[w]:>10}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc493eae",
   "metadata": {},
   "source": [
    "### Term Frequency - Inverse Document Frequency (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c5ae575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TF-IDF:\n",
      "   truck   road  bicycle  drive  highway    car    path\n",
      "0  0.000  0.159   0.0000    0.0    0.000  0.159  0.0000\n",
      "1  0.159  0.000   0.0000    0.0    0.159  0.000  0.0000\n",
      "2  0.000  0.000   0.2386    0.0    0.000  0.000  0.1193\n"
     ]
    }
   ],
   "source": [
    "# Computing TF-IDF\n",
    "df_tf_idf = df_tf.copy()\n",
    "\n",
    "for w in words_set:\n",
    "    for i in range(n_docs):\n",
    "        df_tf_idf[w][i] = df_tf[w][i] * idf[w]\n",
    "\n",
    "print('\\nTF-IDF:')\n",
    "print(df_tf_idf.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7b0f38",
   "metadata": {},
   "source": [
    "## Part-of-Speach (POS) tagging\n",
    "For meaning of POS-tags see: https://pythonexamples.org/nltk-pos-tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5c8c05c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Back', 'RB', 'O'),\n",
      " ('at', 'IN', 'O'),\n",
      " ('the', 'DT', 'B-NP'),\n",
      " ('castle', 'NN', 'I-NP'),\n",
      " (',', ',', 'O'),\n",
      " ('the', 'DT', 'O'),\n",
      " ('Magic', 'NNP', 'O'),\n",
      " ('Mirror', 'NNP', 'O'),\n",
      " ('reveals', 'VBZ', 'O'),\n",
      " ('that', 'IN', 'O'),\n",
      " ('Snow', 'NNP', 'O'),\n",
      " ('White', 'NNP', 'O'),\n",
      " ('is', 'VBZ', 'O'),\n",
      " ('still', 'RB', 'O'),\n",
      " ('living', 'JJ', 'O'),\n",
      " (',', ',', 'O'),\n",
      " ('and', 'CC', 'O'),\n",
      " ('with', 'IN', 'O'),\n",
      " ('the', 'DT', 'B-NP'),\n",
      " ('dwarfs', 'NN', 'I-NP')]\n"
     ]
    }
   ],
   "source": [
    "text = '''Back at the castle, the Magic Mirror reveals that Snow White is still living, and with the dwarfs '''\n",
    "\n",
    "def preprocess(sent):\n",
    "    sent = nltk.word_tokenize(sent)\n",
    "    sent = nltk.pos_tag(sent)\n",
    "    return sent\n",
    "\n",
    "sent = preprocess(text)\n",
    "pattern = 'NP: {<DT>?<JJ>*<NN>}'\n",
    "\n",
    "cp = nltk.RegexpParser(pattern)\n",
    "cs = cp.parse(sent)\n",
    "\n",
    "iob_tagged = tree2conlltags(cs)\n",
    "\n",
    "# Print the POS-tags\n",
    "pprint(iob_tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fcdb69",
   "metadata": {},
   "source": [
    "## Explain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ed0a92",
   "metadata": {},
   "source": [
    "> The POS-tags in the output indicate the grammatical roles of words in the sentence, such as RB (Adverb) for 'Back' and 'still', IN (Preposition or Subordinating Conjunction) for 'at', 'that', and 'with', DT (Determiner) for 'the', NN (Noun, Singular or Mass) for 'castle' and 'dwarfs', and NNP (Proper Noun, Singular) for 'Magic', 'Mirror', 'Snow', and 'White'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1243de",
   "metadata": {},
   "source": [
    "### Jupyter notebook --footer info-- (please always provide this at the end of each submitted notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "017357b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "POSIX\n",
      "Linux | 6.5.0-1025-azure\n",
      "Datetime: 2024-11-27 11:13:26\n",
      "Python Version: 3.11.10\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import platform\n",
    "import socket\n",
    "from platform import python_version\n",
    "from datetime import datetime\n",
    "\n",
    "print('-----------------------------------')\n",
    "print(os.name.upper())\n",
    "print(platform.system(), '|', platform.release())\n",
    "print('Datetime:', datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "print('Python Version:', python_version())\n",
    "print('-----------------------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
